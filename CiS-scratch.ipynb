{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Experimentation Starts Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction Experiments\n",
    "\n",
    "# from flair.embeddings import WordEmbeddings\n",
    "# from flair.embeddings import TransformerWordEmbeddings\n",
    "# from flair.data import Sentence\n",
    "# from scipy.spatial import distance\n",
    "\n",
    "# Pickle Test ==> Success\n",
    "# test = [\"Picle\", \"Testing\", \"Test\"]\n",
    "# pickle_data(test, \"test_pickled_data\")\n",
    "\n",
    "# Unpickle Test ==> Unsucessful\n",
    "# un_test = unpickle_data(\"test_pickled_data\")\n",
    "\n",
    "# secrets[66] try removing forward slash and all, infact remove all punctations and shit, check vulbert\n",
    "# Or replace all singel quotations marks for another\n",
    "# secrets[127]\n",
    "# clean_sec = cleaner(secrets[:100])\n",
    "# Pickle Test ==> Success\n",
    "# test = [\"Picle\", \"Testing\", \"Test\"]\n",
    "# pickle_data(test, \"test_pickled_data\")\n",
    "\n",
    "# Unpickle Test ==> Unsucessful\n",
    "# un_test = unpickle_data(\"test_pickled_data\")\n",
    "\n",
    "# from transformers import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# temp_tokens = tokenizer.__call__(clean_sec, add_special_tokens = True, max_length = 128, return_token_type_ids = True, \n",
    "# padding = \"max_length\", truncation = True, return_attention_mask = True, return_tensors = \"pt\")\n",
    "\n",
    "\n",
    "# # res_tokens = tokenizer.tokenize(clean_sec)\n",
    "# temp_t = temp_tokens[\"input_ids\"]\n",
    "# decoded_str = tokenizer.decode(temp_t)\n",
    "#         args = model_args,\n",
    "\n",
    "# embeddings_data.shape # ==> (5, 24, 768)\n",
    "# vul_X_train_256.shape ==> (3000, 256, 32)\n",
    "# from simpletransformers.config.model_args import ModelArgs\n",
    "\n",
    "# model_args = ModelArgs(max_seq_length = 100)\n",
    "\n",
    "# try removing all \n",
    "# temp = \"temp = \"user\", Passwd: \"password\", Net: \"tcp\", Addr: \"127.0.0.1:3306\"\"\n",
    "# len(secrets[1000:2000])\n",
    "\n",
    "\n",
    "# from transformers import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# from simpletransformers.language_representation import RepresentationModel\n",
    "\n",
    "# model = RepresentationModel(\n",
    "#         model_type = \"bert\",\n",
    "#         model_name = \"bert-base-uncased\",\n",
    "#         use_cuda = False\n",
    "#     )\n",
    "# embeddings_data = model.encode_sentences(benign[4000:5000], combine_strategy = None, batch_size = 1000)\n",
    "\n",
    "# Store data\n",
    "# pickle_data(embeddings_data, \"5-1k-ben\")\n",
    "\n",
    "\n",
    "#---> secrets_embeddings_2: The length of this is not even with the others, for what reason exactly, don't know....\n",
    "#---> benign_embeddings_(c2: This is cool though.\n",
    "\n",
    "## Verify length \n",
    "# secrets_embeddings_1 (seq = 128, dimensional_vectors = 768)\n",
    "# secrets_embeddings_2 (seq = 127, dimensional_vectors = 768) # Add 768, vectors of zeros, to all the 127 sequence making them 128 (768,) numpy.float32 \n",
    "# secrets_embeddings_3 (seq = 128, dimensional_vectors = 768)\n",
    "# secrets_embeddings_4 (seq = 128, dimensional_vectors = 768)\n",
    "# secrets_embeddings_5 (seq = 128, dimensional_vectors = 768)\n",
    "\n",
    "# benign_embeddings_1 (seq = 128, dimensional_vectors = 768)\n",
    "# benign_embeddings_2 (seq = 128, dimensional_vectors = 768)\n",
    "# benign_embeddings_3 (seq = 128, dimensional_vectors = 768)\n",
    "# benign_embeddings_4 (seq = 128, dimensional_vectors = 768)\n",
    "# benign_embeddings_5 (seq = 128, dimensional_vectors = 768)\n",
    "\n",
    "# # This is what flattens spectre vulnerbility dimensional data to fit non deep learning models\n",
    "\n",
    "# secrets_embeddings_5 = list(unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/features/secret_embeddings/5-1k-sec\"))\n",
    "# # len(vul_X_train_256) ==> 3000 (observational length), 256 (sequence length), 32 (dimensional vectors)\n",
    "# train_flat = vul_X_train_256.reshape(3000, 256 * 32) # flaterns dims\n",
    "\n",
    "\n",
    "# # replace all something here \"r\"[^\\w\\s]\" with nothing here \"\" # re.sub(r\"[^\\w\\s]\", \"\", obs.strip())) \n",
    "\n",
    "# temp = [\" !!!user:qfcruihk@tcp(localhost:5555)/dbname?charset=utf8mb4,utf8&tls=skip-verify\", \"user:ashpnftu@tcp(localhost:5555)/dbname?charset=utf8&tls=true  \"]\n",
    "# out = []\n",
    "# [out.append(re.sub(r\"[^\\w\\s]\", \"\", obs.strip())) for obs in temp]\n",
    "# out\n",
    "\n",
    "# temp = [\" user:qfcruihk@tcp(localhost:5555)/dbname?charset=utf8mb4,utf8&tls=skip-verify\", \"user:ashpnftu@tcp(localhost:5555)/dbname?charset=utf8&tls=true  \"]\n",
    "# for obs in temp:\n",
    "#      print(obs.strip())\n",
    "\n",
    "from simpletransformers.config.model_args import ModelArgs\n",
    "from simpletransformers.language_representation import RepresentationModel\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from typing import List, Tuple\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.functional import Tensor\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import sampler, DataLoader, TensorDataset\n",
    "from typing import Dict, List, Sequence, Set, Text, Tuple, Union\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, fbeta_score, accuracy_score\n",
    "\n",
    "\n",
    "from torch.utils.data import sampler, DataLoader, TensorDataset\n",
    "from typing import Dict, List, Sequence, Set, Text, Tuple, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_validate\n",
    "from imblearn.metrics import geometric_mean_score \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from math import sqrt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import time\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def pickle_data(data, file_name):\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "        \n",
    "def unpickle_data(data):\n",
    "    with open(data, \"rb\") as file:\n",
    "        loaded = pickle.load(file)\n",
    "    return loaded\n",
    "\n",
    "def reader(root: str, file: str):\n",
    "    with open(os.path.join(root, file), \"r\") as f: return f.readlines()\n",
    "    \n",
    "def cleaner(data: List[str]):\n",
    "    out = []\n",
    "    [out.append(re.sub(r\"[^\\w\\s]\", \"\", obs.strip())) for obs in data]\n",
    "    return out\n",
    "\n",
    "\n",
    "# secrets = reader(\"/Users/Gabriel/Projects/Context-in-Secrets/data/benchmark/\", \"secrets.txt\")[:5000]\n",
    "# benign = reader(\"/Users/Gabriel/Projects/Context-in-Secrets/data/benchmark/\", \"benign.txt\")[:5000]\n",
    "\n",
    "# # Store data \n",
    "# pickle_data(embeddings_data, \"benign-5k\") # (embeddings_data = rawdata, \"benign-5k\" = filename)\n",
    "\n",
    "# # secrets = reader(\"/Users/Gabriel/Projects/Context-in-Secrets/data/benchmark/\", \"secrets.txt\")[:5000]\n",
    "# benign = reader(\"/Users/Gabriel/Projects/Context-in-Secrets/data/benchmark/\", \"benign.txt\")[:5000]\n",
    "\n",
    "# secrets_emb = unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/features/secret_embeddings/1-1k-sec\")\n",
    "# benign_emb = unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/features/secret_embeddings/1-1k-sec\")\n",
    "\n",
    "# model = RepresentationModel(\n",
    "#         model_type = \"bert\",\n",
    "#         model_name = \"bert-base-uncased\",\n",
    "#         use_cuda = False\n",
    "#     )\n",
    "# embeddings_data = model.encode_sentences(benign, combine_strategy = \"mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>FileID</th>\n",
       "      <th>Domain</th>\n",
       "      <th>RepoName</th>\n",
       "      <th>FilePath</th>\n",
       "      <th>LineStart:LineEnd</th>\n",
       "      <th>GroundTruth</th>\n",
       "      <th>WithWords</th>\n",
       "      <th>ValueStart</th>\n",
       "      <th>ValueEnd</th>\n",
       "      <th>...</th>\n",
       "      <th>CharacterSet</th>\n",
       "      <th>CryptographyKey</th>\n",
       "      <th>PredefinedPattern</th>\n",
       "      <th>VariableNameType</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Length</th>\n",
       "      <th>Base64Encode</th>\n",
       "      <th>HexEncode</th>\n",
       "      <th>URLEncode</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Id, FileID, Domain, RepoName, FilePath, LineStart:LineEnd, GroundTruth, WithWords, ValueStart, ValueEnd, InURL, InRuntimeParameter, CharacterSet, CryptographyKey, PredefinedPattern, VariableNameType, Entropy, Length, Base64Encode, HexEncode, URLEncode, Category]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 22 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metadata = pd.read_csv(\"/Users/Gabriel/Projects/CADE/datasets/exp/s-meta/ffea718f.csv\")\n",
    "# metadata = metadata.loc[(metadata['GroundTruth'].isin([\"T\"])) & (metadata['Category'].isin([\"Password\"]))]\n",
    "\n",
    "metadata = pd.read_csv(\"/Users/Gabriel/Downloads/CodeTest/s-meta/0f133e09.csv\")\n",
    "\n",
    "metadata[\"FilePath\"] = metadata[\"FilePath\"].apply(lambda fp: fp.split(\"/\")[-1])\n",
    "metadata[\"LineStart:LineEnd\"] =  metadata[\"LineStart:LineEnd\"].apply(lambda x: x.split(\":\")[0])\n",
    "metadata = metadata.loc[(metadata['GroundTruth'].isin([\"T\"]))]\n",
    "metadata.loc[(metadata['Category'].isin([\"Generic Secret\"]))]\n",
    "\n",
    "metadata = pd.read_csv(\"/Users/Gabriel/Projects/CADE/datasets/exp/s-meta/0de15ccf.csv\")\n",
    "metadata[\"FilePath\"], metadata[\"LineStart:LineEnd\"] = metadata[\"FilePath\"].apply(lambda fp: fp.split(\"/\")[-1]), metadata[\"LineStart:LineEnd\"].apply(lambda x: x.split(\":\")[0])\n",
    "# metadata = metadata.loc[(metadata['GroundTruth'].isin([\"T\"])) & (metadata['Category'].isin([\"Password\"]))]\n",
    "metadata = metadata.loc[(metadata['GroundTruth'].isin([\"T\"]))]\n",
    "metadata = metadata.loc[(metadata['Category'].isin([ \"Password\"]))]\n",
    "metadata\n",
    "\n",
    "metadata = pd.read_csv(\"/Users/Gabriel/Projects/CADE/datasets/exp/s-meta/0f133e09.csv\")\n",
    "metadata[\"FilePath\"], metadata[\"LineStart:LineEnd\"] = metadata[\"FilePath\"].apply(lambda fp: fp.split(\"/\")[-1]), metadata[\"LineStart:LineEnd\"].apply(lambda x: x.split(\":\")[0])\n",
    "# metadata = metadata.loc[(metadata['GroundTruth'].isin([\"T\"])) & (metadata['Category'].isin([\"Password\"]))]\n",
    "metadata = metadata.loc[(metadata['GroundTruth'].isin([\"T\"]))]\n",
    "# metadata = metadata.loc[(metadata['Category'].isin([\"Password\"]))]\n",
    "metadata\n",
    "\n",
    "metadata = metadata.loc[(metadata['GroundTruth'].isin(filt)) & (metadata['Category'].isin(password))]\n",
    "positive = \"T\"\n",
    "negative = \"F\"\n",
    "partial_positive = \"Template\"\n",
    "filt = [positive]\n",
    "# metadata = metadata.loc[metadata[\"GroundTruth\"].isin(filt)]\n",
    "metadata_istrue = metadata.loc[metadata[\"GroundTruth\"].isin(filt), metadata[\"Category\"].isin(password)]\n",
    "metadata_istrue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>FileID</th>\n",
       "      <th>Domain</th>\n",
       "      <th>RepoName</th>\n",
       "      <th>FilePath</th>\n",
       "      <th>LineStart:LineEnd</th>\n",
       "      <th>GroundTruth</th>\n",
       "      <th>WithWords</th>\n",
       "      <th>ValueStart</th>\n",
       "      <th>ValueEnd</th>\n",
       "      <th>...</th>\n",
       "      <th>CharacterSet</th>\n",
       "      <th>CryptographyKey</th>\n",
       "      <th>PredefinedPattern</th>\n",
       "      <th>VariableNameType</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Length</th>\n",
       "      <th>Base64Encode</th>\n",
       "      <th>HexEncode</th>\n",
       "      <th>URLEncode</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>35014</td>\n",
       "      <td>c13abbcc</td>\n",
       "      <td>GitHub</td>\n",
       "      <td>0b0a8cd6</td>\n",
       "      <td>e.json</td>\n",
       "      <td>35</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Any</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Secret</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>Password</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>37324</td>\n",
       "      <td>c13abbcc</td>\n",
       "      <td>GitHub</td>\n",
       "      <td>0b0a8cd6</td>\n",
       "      <td>e.json</td>\n",
       "      <td>164</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Any</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Secret</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>Password</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>42779</td>\n",
       "      <td>29890666</td>\n",
       "      <td>GitHub</td>\n",
       "      <td>0b0a8cd6</td>\n",
       "      <td>a.py</td>\n",
       "      <td>25</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>42.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Any</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Secret</td>\n",
       "      <td>2.81</td>\n",
       "      <td>7.0</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>Password</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id    FileID  Domain  RepoName FilePath LineStart:LineEnd GroundTruth  \\\n",
       "23  35014  c13abbcc  GitHub  0b0a8cd6   e.json                35           T   \n",
       "25  37324  c13abbcc  GitHub  0b0a8cd6   e.json               164           T   \n",
       "27  42779  29890666  GitHub  0b0a8cd6     a.py                25           T   \n",
       "\n",
       "   WithWords  ValueStart  ValueEnd  ... CharacterSet CryptographyKey  \\\n",
       "23         F        11.0      15.0  ...          Any             NaN   \n",
       "25         F        11.0      15.0  ...          Any             NaN   \n",
       "27         T        42.0      49.0  ...          Any             NaN   \n",
       "\n",
       "   PredefinedPattern  VariableNameType  Entropy Length  Base64Encode  \\\n",
       "23               NaN            Secret     2.00    4.0             F   \n",
       "25               NaN            Secret     2.00    4.0             F   \n",
       "27               NaN            Secret     2.81    7.0             F   \n",
       "\n",
       "    HexEncode URLEncode  Category  \n",
       "23          F         F  Password  \n",
       "25          F         F  Password  \n",
       "27          F         F  Password  \n",
       "\n",
       "[3 rows x 22 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "password = [\"Password\"] # category\n",
    "metadata_istrue.loc[metadata[\"Category\"].isin(password)] # search is in alternative for one # next thing to do is to count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['userqfcruihktcplocalhost5555dbnamecharsetutf8mb4utf8tlsskipverify']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# temp = [\" !!!user:qfcruihk@tcp(localhost:5555)/dbname?charset=utf8mb4,utf8&tls=skip-verify\"]\n",
    "# out = []\n",
    "# [out.append(re.sub(r\"[^\\w\\s]\", \"\", obs.strip())) for obs in temp]\n",
    "# out\n",
    "\n",
    "# from transformers import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# temp_tokens = tokenizer.__call__(clean_sec, add_special_tokens = True, max_length = 128, return_token_type_ids = True, \n",
    "# padding = \"max_length\", truncation = True, return_attention_mask = True, return_tensors = \"pt\")\n",
    "\n",
    "# testing = \"user = models.User.objects.create(username='joe', password='secret'\"\n",
    "# testing1 = \"user = authenticate(username='john', password='secret')\"\n",
    "# testing3 = \"pwd: 'masked pwd'\"\n",
    "\n",
    "# main_testing = \"pwd: 'foo'\" # pass to model\n",
    "\n",
    "# res = tokenizer.tokenize(str(main_testing))\n",
    "\n",
    "# print(res)\n",
    "# # print(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer \n",
    "testing4 = ['userqf cruihktcplo calhost5555dbnamecharsetut f8mb4utf8tlsskipverif y']\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "temp_tokens = tokenizer.__call__(testing4, add_special_tokens = True, max_length = 128, return_token_type_ids = False, \n",
    "padding = \"max_length\", truncation = True, return_attention_mask = True, return_length = True, return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "password.txt\n",
      "generic_secret.txt\n",
      "private_key.txt\n",
      "generic_token.txt\n",
      "predefined_pattern.txt\n",
      "auth_key_token.txt\n",
      "seed_salt_nonce.txt\n",
      "other.txt\n"
     ]
    }
   ],
   "source": [
    "category = {\n",
    "\t\t\"password\" : [\"Password\"],\n",
    "\t\t\"generic_secret\" : [\"Generic Secret\"],\n",
    "\t\t\"private_key\" : [\"Private Key\"],\n",
    "\t\t\"generic_token\" : [\"Generic Token\"],\n",
    "\t\t\"predefined_pattern\" : [\"Predefined Pattern\"],\n",
    "\t\t\"auth_key_token\" : [\"Authentication Key & Token\"],\n",
    "\t\t\"seed_salt_nonce\" : [\"Seed, Salt, Nonce\"],\n",
    "\t\t\"other\" : [\"Other\"]\n",
    "}\n",
    "\n",
    "for data in category.keys():\n",
    "    print(data + \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Password']\n",
      "['Generic Secret']\n",
      "['Private Key']\n",
      "['Generic Token']\n",
      "['Predefined Pattern']\n",
      "['Authentication Key & Token']\n",
      "['Seed, Salt, Nonce']\n",
      "['Other']\n"
     ]
    }
   ],
   "source": [
    "for val in category.values():\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', '##wd', ':', \"'\", 'foo', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "# \"pwd: 'foo'\" ==> ['p', '##wd', ':', \"'\", 'foo', \"'\"]\n",
    "\n",
    "\n",
    "# [[CLS] = 101, \"p\" = 1052, \"##wd\" = [21724], \":\" = 1024, \"'\" = [1005], \"foo\" = [29379], \"'\" = [1005], [SEP] = [102]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets_emb = unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/ID-5K/secrets-5k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(secrets_emb[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregated observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Try increasing the dimensions of non-mean data later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is mean \n",
    "\n",
    "secrets_emb = unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/ID-5K/secrets-5k\")\n",
    "benign_emb = unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/ID-5K/benign-5k\")\n",
    "\n",
    "secret_labels = [1 for _ in range(5000)]\n",
    "benign_labels = [0 for _ in range(5000)]\n",
    "X = np.concatenate((secrets_emb, benign_emb))\n",
    "\n",
    "y = secret_labels + benign_labels\n",
    "y  = np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encrytion and shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secrets = reader(\"/Users/Gabriel/Projects/Context-in-Secrets/data/benchmark/\", \"secrets.txt\")[:5000]\n",
    "benign = reader(\"/Users/Gabriel/Projects/Context-in-Secrets/data/benchmark/\", \"benign.txt\")[:5000]\n",
    "\n",
    "secrets_out = []\n",
    "# [secrets_out.append(re.sub(r\"[^\\w\\s]\", \"\", obs.strip())) for obs in secrets]\n",
    "[secrets_out.append(re.sub(r\"[^\\w\\s]\", \"\", obs).strip()) for obs in secrets]\n",
    "\n",
    "\n",
    "benign_out = []\n",
    "[benign_out.append(re.sub(r\"[^\\w\\s]\", \"\", obs).strip()) for obs in benign]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original string:  Password = \"fake2\",\n",
      "\n",
      "encrypted string:  b'gAAAAABjaWJx26X-VDrOQpmi5Py10kPK82JeBqHnp9EHxIYN1O9d-sdhyr3imHHn-B6xLJ3YlrF-yb56TaECSl2PH7mGNxFTebIA2jf7oojPxc7wIoSRxUk='\n"
     ]
    }
   ],
   "source": [
    "from cryptography.fernet import Fernet\n",
    "\n",
    "key = Fernet.generate_key()\n",
    "\n",
    "fernet = Fernet(key)\n",
    "\n",
    "encMessage = fernet.encrypt(secrets_exp[0].encode())\n",
    "\n",
    "print(\"original string: \", secrets_exp[0])\n",
    "print(\"encrypted string: \", encMessage)\n",
    "\n",
    "decMessage = fernet.decrypt(encMessage).decode()\n",
    "print(\"decrypted string: \", decMessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encrypted_secrets = []\n",
    "\n",
    "# The process and parsing the data to information on a specific field is not even yet reliant on the proporsition that the requirement must first be meet and aggressive\n",
    "# and  reliance of the next task or input specified or given\n",
    "# for obs in secrets:\n",
    "#     encrypted_secrets.append(fernet.encrypt(obs.encode()))\n",
    "    \n",
    "# benign_secrets = []\n",
    "# for obs in benign:\n",
    "#     benign_secrets.append(fernet.encrypt(obs.encode()))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_data(encrypted_secrets, \"encrypted_secrets\")\n",
    "# pickle_data(benign_secrets, \"encrypted_benign\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['i', 'am', 'gh', 'H'], dtype='<U2')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not deecoded, but convert to strings\n",
    "\n",
    "# decoded_encrypted_secrets = []\n",
    "\n",
    "# for data in encrypted_secrets:\n",
    "#     decoded_encrypted_secrets.append(data.decode(\"utf-8\"))\n",
    "\n",
    "    \n",
    "# decoded_encrypted_benign = []\n",
    "\n",
    "# for data in benign_secrets:\n",
    "#     decoded_encrypted_benign.append(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTextRepresentation: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTextRepresentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTextRepresentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# model = RepresentationModel(\n",
    "#         model_type = \"bert\",\n",
    "#         model_name = \"bert-base-uncased\",\n",
    "#         use_cuda = False\n",
    "#     )\n",
    "# embeddings_secret_data = model.encode_sentences(decoded_encrypted_secrets, combine_strategy = \"mean\")\n",
    "\n",
    "model = RepresentationModel(\n",
    "        model_type = \"bert\",\n",
    "        model_name = \"bert-base-uncased\",\n",
    "        use_cuda = False\n",
    "    )\n",
    "embeddings_benign_data = model.encode_sentences(decoded_encrypted_benign, combine_strategy = \"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_data(embeddings_secret_data, \"encrypted_secrets_embeddings\")\n",
    "# pickle_data(embeddings_benign_data, \"encrypted_benign_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(embeddings_secret_data)\n",
    "# len(embeddings_benign_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(encrypted_secrets)\n",
    "# len(benign_secrets)\n",
    "# secret_labels = [1 for _ in range(len(embeddings_secret_data))]\n",
    "# benign_labels = [0 for _ in range(len(embeddings_benign_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_labels = np.array([1 for _ in range(len(embeddings_secret_data))])\n",
    "benign_labels = np.array([0 for _ in range(len(embeddings_benign_data))])\n",
    "\n",
    "X = np.concatenate((embeddings_secret_data, embeddings_benign_data))\n",
    "y = np.concatenate((secret_labels, benign_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# knn = KNeighborsClassifier()\n",
    "# knn.fit(X_train, y_train)\n",
    "\n",
    "# mlp = MLPClassifier(random_state = 10, max_iter = 300).fit(X_train, y_train)\n",
    "# mlp = MLPClassifier(random_state = 10, max_iter = 20000).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# svc = svm.SVC(kernel='linear', gamma='auto', C=2).fit(X_train, y_train)\n",
    "# lsvc = svm.LinearSVC(max_iter=20000).fit(X_train, y_train)\n",
    "\n",
    "# lr = LogisticRegression(solver='lbfgs', max_iter=20000).fit(X_train, y_train)\n",
    "# rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Th:\n",
    "    def hello():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Th'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Th().__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GaussianNB'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GaussianNB().__class__.__name__  # GaussianNB \n",
    "# KNeighborsClassifier().__class__.__name__  # KNeighborsClassifier\n",
    "# svm.LinearSVC(max_iter = 20000).__class__.__name__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GaussianNB is the name\n",
    "GaussianNB() is the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n",
      "[3, 4]\n"
     ]
    }
   ],
   "source": [
    "dark = {\"one\" : [1, 2], \"two\" : [3, 4]}\n",
    "\n",
    "for m in dark:\n",
    "    print(dark[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'f1_score' : 'f1', 'fb': make_scorer(fbeta_score, beta=0.5), 'precision': 'precision', 'recall' : 'recall', \n",
    "           'specificity' : make_scorer(recall_score, pos_label=0), 'gmean':  make_scorer(geometric_mean_score, greater_is_better=True, average='binary')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "# lr = LogisticRegression(solver='lbfgs', max_iter=20000).fit(X_train, y_train)\n",
    "\n",
    "# knn = KNeighborsClassifier()\n",
    "# knn.fit(X_train, y_train)\n",
    "\n",
    "# scores_rf = cross_validate(rf, X_train, y_train, scoring=scoring, cv = 10)\n",
    "# scores_lr = cross_validate(lr, X_train, y_train, scoring=scoring, cv = 10)\n",
    "# scores_knn = cross_validate(knn, X_train, y_train, scoring=scoring, cv = 10)\n",
    "\n",
    "scores_nb = cross_validate(nb, X_train, y_train, scoring=scoring, cv = 10)\n",
    "\n",
    "# sorted(scores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.6327560630004139\n",
      "Fb: 0.6235671888593669\n",
      "Pre: 0.6176373757586513\n",
      "Rec: 0.6489201995012468\n",
      "Spec: 0.5978145363408522\n",
      "GM: 0.6226522050998458\n",
      "Avg: 0.6238912614267295\n",
      "Fit_time: 76.92401089668274\n"
     ]
    }
   ],
   "source": [
    "# organization:\n",
    "\n",
    "F1 = np.mean(scores_nb['test_f1_score'])\n",
    "Fb = np.mean(scores_nb['test_fb'])\n",
    "Pre = np.mean(scores_nb['test_precision'])\n",
    "Rec = np.mean(scores_nb['test_recall'])\n",
    "Spec = np.mean(scores_nb['test_specificity'])\n",
    "GM = np.mean(scores_nb['test_gmean'])\n",
    "\n",
    "print(f\"F1: {F1}\")\n",
    "print(f\"Fb: {Fb}\")\n",
    "print(f\"Pre: {Pre}\")\n",
    "print(f\"Rec: {Rec}\")\n",
    "print(f\"Spec: {Spec}\")\n",
    "print(f\"GM: {GM}\")\n",
    "\n",
    "print(f\"Avg: {np.mean([F1, Fb, Pre, Rec, Spec, GM])}\")\n",
    "print(f\"Fit_time: {np.mean(scores['fit_time'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting. \n",
      "Time: 0.028191089630126953 seconds\n"
     ]
    }
   ],
   "source": [
    "# ONLY TIME PREDS\n",
    "start_time = time.time()\n",
    "y_pred = nb.predict(X_test) \n",
    "print(f\"Finished predicting. \\nTime: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one': None, 'two': None, 'three': None}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifiers = {\"one\" : [1, 2, 3], \"two\" : [4, 5, 6], \"three\" : [7, 8, 9]}\n",
    "# {model : classifiers[model].append(50) for model in classifiers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'one': [1, 2, 3], 'two': [4, 5, 6], 'three': [7, 8, 9]}\n"
     ]
    }
   ],
   "source": [
    "print(tempva())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f1_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a7c030a2efc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mF1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mFb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfbeta_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mPre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mRec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mSpec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f1_score' is not defined"
     ]
    }
   ],
   "source": [
    "F1 = f1_score(y_test, y_pred)\n",
    "Fb = fbeta_score(y_test, y_pred, beta=0.5)\n",
    "Pre = precision_score(y_test, y_pred)\n",
    "Rec = recall_score(y_test, y_pred)\n",
    "Spec = recall_score(y_test, y_pred, pos_label = 0)\n",
    "GM = geometric_mean_score(y_test, y_pred, average='binary')\n",
    "\n",
    "print(f\"F1: {F1}\")\n",
    "print(f\"Fb: {Fb}\")\n",
    "print(f\"Pre: {Pre}\")\n",
    "print(f\"Rec: {Rec}\")\n",
    "print(f\"Spec: {Spec}\")\n",
    "print(f\"g_mean: {GM}\")\n",
    "print(f\"Avg: {np.mean([F1, Fb, Pre, Rec, Spec, GM])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP-CV\n",
    "# F1: 0.911536998414963\n",
    "# Fb: 0.9091507081601895\n",
    "# Pre: 0.9077526732181177\n",
    "# Rec: 0.9163048628428927\n",
    "# Spec: 0.905718045112782\n",
    "# GM: 0.9107093143901563\n",
    "# Avg: 0.9101954336898502\n",
    "# Fit_time: 10.009365773200988\n",
    "# f1_score = np.mean(scores['test_f1_score'])\n",
    "\n",
    "# MLP-Test\n",
    "\n",
    "# F1: 0.9123685528292438\n",
    "# Fb: 0.9120945134160994\n",
    "# Pre: 0.9119119119119119\n",
    "# Rec: 0.9128256513026052\n",
    "# Spec: 0.9121756487025948\n",
    "# g_mean: 0.912500592125464\n",
    "# Avg: 0.9123128117146533"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing with state-of-the-art tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cred Sweeper: 0.8608666666666668\n",
      "Credential Digger: 0.09666666666666668\n",
      "Detect Secrets: 0.2431\n",
      "Git Leaks: 0.36779999999999996\n",
      "Shhgit: 0.23906666666666668\n",
      "Truffle Hog3: 0.09223333333333333\n",
      "Wraith: 0.20993333333333333\n",
      "This Work: 0.9123333333333333\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cred Sweeper: {np.mean([0.9165, 0.8075, 0.8586])}\")\n",
    "print(f\"Credential Digger: {np.mean([0.0895, 0.1045, 0.0960])}\")\n",
    "print(f\"Detect Secrets: {np.mean([0.1415, 0.3814, 0.2064])}\")\n",
    "print(f\"Git Leaks: {np.mean([0.5255, 0.2443, 0.3336])}\")\n",
    "print(f\"Shhgit: {np.mean([0.5188, 0.0720, 0.1264])}\")\n",
    "print(f\"Truffle Hog3: {np.mean([0.2500, 0.0091, 0.0176])}\")\n",
    "print(f\"Wraith: {np.mean([0.2246, 0.1959, 0.2093])}\")\n",
    "\n",
    "print(f\"This Work: {np.mean([0.9119, 0.9128, 0.9123])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-aggregated Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# secrets_embeddings_1 = unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/features/secret_embeddings/1-1k-sec\")\n",
    "# # secrets_embeddings_2 = unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/features/secret_embeddings/2-1k-sec\")\n",
    "# secrets_embeddings_3 = unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/features/secret_embeddings/3-1k-sec\")\n",
    "# secrets_embeddings_4 = unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/features/secret_embeddings/4-1k-sec\")\n",
    "# secrets_embeddings_5 = unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/features/secret_embeddings/5-1k-sec\")\n",
    "\n",
    "# benign_embeddings_1 = unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/features/benign_embeddings/1-1k-ben\")\n",
    "# # benign_embeddings_2 = unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/features/benign_embeddings/2-1k-ben\")\n",
    "# benign_embeddings_3 = unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/features/benign_embeddings/3-1k-ben\")\n",
    "# benign_embeddings_4 = unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/features/benign_embeddings/4-1k-ben\")\n",
    "# benign_embeddings_5 = unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/features/benign_embeddings/5-1k-ben\")\n",
    "\n",
    "# # 8K observations (4K Secret 4K benign)\n",
    "# secrets_embeddings = np.concatenate((exp_secrets_embeddings_1, secrets_embeddings_3, exp_secrets_embeddings_4, exp_secrets_embeddings_5))\n",
    "# benign_embeddings = np.concatenate((benign_embeddings_1, benign_embeddings_3, benign_embeddings_4, benign_embeddings_5))\n",
    "\n",
    "# X = np.concatenate((secrets_embeddings, benign_embeddings))\n",
    "\n",
    "# secret_labels = [1 for _ in range(len(secrets_embeddings))]\n",
    "# benign_labels = [0 for _ in range(len(benign_embeddings))]\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 4)\n",
    "\n",
    "# secrets_embeddings = np.concatenate((secrets_embeddings_1, secrets_embeddings_2, secrets_embeddings_3, secrets_embeddings_4, secrets_embeddings_5))\n",
    "# secrets_embeddings = np.vstack((secrets_embeddings_1, secrets_embeddings_2, secrets_embeddings_3, secrets_embeddings_4, secrets_embeddings_5))\n",
    "\n",
    "# #========== Load Data ========================================\n",
    "# secrets_embeddings = np.array(unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/bert-embeddings/secrets_embeddings\"))\n",
    "# benign_embeddings = np.array(unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/bert-embeddings/benign_embeddings\"))\n",
    "# X = np.concatenate((secrets_embeddings, benign_embeddings))\n",
    "\n",
    "# # secrets_embeddings = np.array(secrets_embeddings)\n",
    "# # benign_embeddings = np.array(benign_embeddings)\n",
    "\n",
    "# #========== Map Labels ========================================\n",
    "# secret_labels = np.array([1 for _ in range(len(secrets_embeddings))])\n",
    "# benign_labels = np.array([0 for _ in range(len(benign_embeddings))])\n",
    "# y = np.concatenate((secret_labels, benign_labels))\n",
    "\n",
    "# #========== Concatinate data ========================================\n",
    "# # y = np.concatenate((secret_labels, secret_labels))\n",
    "\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X,y, stratify = y, random_state=4)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Experimenting with one-dimensional data\n",
    "\n",
    "# secret_labels = [1 for _ in range(5000)]\n",
    "# benign_labels = [0 for _ in range(5000)]\n",
    "# y = secret_labels + benign_labels\n",
    "\n",
    "secrets_emb = unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/ID-5K/secrets-5k\")\n",
    "benign_emb = unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/ID-5K/benign-5k\")\n",
    "\n",
    "secret_labels = np.array([1 for _ in range(len(secrets_emb))])\n",
    "benign_labels = np.array([0 for _ in range(len(benign_emb))])\n",
    "\n",
    "\n",
    "X = np.concatenate((secrets_emb, benign_emb))\n",
    "y = np.concatenate((secret_labels, benign_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't use starred expression here (<ipython-input-98-25102b9d3231>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-98-25102b9d3231>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    *res\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't use starred expression here\n"
     ]
    }
   ],
   "source": [
    "dummy = \"hi\"\n",
    "onep = \"It is well\"\n",
    "twop = \"It is fantastic\"\n",
    "threep = \"It is not well\"\n",
    "\n",
    "res = ((onep, twop) if dummy == \"hi\" else threep)\n",
    "\n",
    "*res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-aee9e654e501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"I am good\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "hell = (\"I am good\")\n",
    "\n",
    "on, two = hell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">_ training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 10000])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-70b47b9b02c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0mtt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m \u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;31m# tt.test()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;31m# tt.store_metrics()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-70b47b9b02c7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIS_NET\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCRITERION\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3054\u001b[0m         \u001b[0mreduction_enum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3055\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3056\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   3057\u001b[0m             \u001b[0;34m\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3058\u001b[0m             \u001b[0;34m\"Please ensure they have the same size.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 10000])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "# #========== Load Data ========================================\n",
    "# secrets_embeddings = np.array(unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/bert-embeddings/secrets_embeddings\"))\n",
    "# benign_embeddings = np.array(unpickle_data(\"/Users/Gabriel/Projects/Context-in-Secrets/src/bert-embeddings/benign_embeddings\"))\n",
    "# X = np.concatenate((secrets_embeddings, benign_embeddings))\n",
    "\n",
    "# secrets_embeddings = np.array(secrets_embeddings)\n",
    "# benign_embeddings = np.array(benign_embeddings)\n",
    "\n",
    "#========== Map Labels ========================================\n",
    "# secret_labels = np.array([1 for _ in range(len(secrets_embeddings))])\n",
    "# benign_labels = np.array([0 for _ in range(len(benign_embeddings))])\n",
    "# y = np.concatenate((secret_labels, benign_labels))\n",
    "\n",
    "#========== Concatinate data ========================================\n",
    "# y = np.concatenate((secret_labels, secret_labels))\n",
    "\n",
    "\n",
    "######## Regular Classifier (80 - 20%)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "######## Convolutional Neural Networks (80 - 10%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=4)\n",
    "\n",
    "X_val = X_train[8000:]\n",
    "y_val = y_train[8000:]\n",
    "\n",
    "X_train = X_train[:8000]\n",
    "y_train = y_train[:8000]\n",
    "\n",
    "loader_train = DataLoader(TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train)), shuffle = True, batch_size = 32)\n",
    "loader_val = DataLoader(TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val)), shuffle = True, batch_size = 32)\n",
    "loader_test = DataLoader(TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test)), shuffle = True, batch_size = 32)\n",
    "\n",
    "import argparse\n",
    "torch.manual_seed(400)\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple\n",
    "\n",
    "class Config:\n",
    "\tdef __init__(\n",
    "\t\t\t\t self, \n",
    "\t\t\t\t epochs: int = 3, \n",
    "\t\t\t\t learning_rate: float = 0.001, \n",
    "\t\t\t\t betas: Tuple[float] = (0.5, 0.999),\n",
    "\t\t\t\t label_num: int = 1, \n",
    "\t\t\t\t in_channels: Tuple = 1,\n",
    "\t\t\t\t out_channels: Tuple = 1, \n",
    "\t\t\t\t in_features: int = 10, \n",
    "\t\t\t\t out_features: int = 300,\n",
    "\t\t\t\t drop_out_prob: float = 0.25,\n",
    "\t\t\t\t kernel_size: int = 3, \n",
    "\t\t\t\t stride: int = 1, \n",
    "# \t\t\t\t padding: List[Tuple] = [(1, 0), (2, 0), (3, 0)], \n",
    "\t\t\t\t padding: List[Tuple] = [1, 2, 3], \n",
    "# \t\t\t\t batch_size: int = 32, \n",
    "\t\t\t\t max_pool = 10\n",
    "# \t\t\t\t max_pool = (10, 10)\n",
    "\t\t\t\t ) -> None:\n",
    "\t\tself.EPOCHS = epochs\n",
    "\t\tself.LEARNING_RATE = learning_rate\n",
    "\t\tself.BETAS = betas\n",
    "\t\tself.LABEL_NUM = label_num\n",
    "\t\tself.IN_CHANNELS = in_channels \n",
    "\t\tself.IN_FEATURES = in_features \n",
    "\t\tself.OUT_FEATURES = out_features\n",
    "\t\tself.DROP_OUT_PROB = drop_out_prob\n",
    "\t\tself.OUT_CHANNELS = out_channels\n",
    "\t\tself.KERNEL_SIZE = kernel_size\n",
    "\t\tself.STRIDE = stride\n",
    "\t\tself.PADDING = padding\n",
    "# \t\tself.BATCH_SIZE = batch_size\n",
    "\t\tself.MAX_POOL = max_pool\n",
    "\n",
    "class CIS(nn.Module):\n",
    "    def __init__(self, task):\n",
    "        super(CIS, self).__init__()\n",
    "        self.CONFIG = Config()\n",
    "        self.task = task\n",
    "        self.clf_layer = nn.Sigmoid()\n",
    "\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Conv1d(self.CONFIG.IN_CHANNELS, self.CONFIG.OUT_CHANNELS, self.CONFIG.KERNEL_SIZE, self.CONFIG.STRIDE, self.CONFIG.PADDING[0]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveMaxPool1d(self.CONFIG.MAX_POOL)\n",
    "        )\n",
    "\n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Conv1d(self.CONFIG.IN_CHANNELS, self.CONFIG.OUT_CHANNELS, self.CONFIG.KERNEL_SIZE, self.CONFIG.STRIDE, self.CONFIG.PADDING[1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveMaxPool1d(self.CONFIG.MAX_POOL)\n",
    "        )\n",
    "\n",
    "        self.l3 = nn.Sequential(\n",
    "            nn.Conv1d(self.CONFIG.IN_CHANNELS, self.CONFIG.OUT_CHANNELS, self.CONFIG.KERNEL_SIZE, self.CONFIG.STRIDE, self.CONFIG.PADDING[2]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveMaxPool2d(self.CONFIG.MAX_POOL)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(self.CONFIG.IN_FEATURES * self.CONFIG.IN_FEATURES, self.CONFIG.OUT_FEATURES)\n",
    "        self.fc2_bin = nn.Linear(self.CONFIG.OUT_FEATURES, self.CONFIG.LABEL_NUM)\n",
    "        self.fc2_mult = nn.Linear(self.CONFIG.OUT_FEATURES, 8)\n",
    "        self.dropout = nn.Dropout(self.CONFIG.DROP_OUT_PROB)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.clf_task = (self.fc2_bin, self.clf_layer if re.match(task, \"binary\", re.IGNORECASE) self.fc2_mult)\n",
    "            \n",
    "        self.model = nn.Sequential(\n",
    "            self.l1,\n",
    "            self.l2,\n",
    "            self.l3,\n",
    "            self.flatten,\n",
    "            self.dropout,\n",
    "            self.fc1,\n",
    "            self.dropout,\n",
    "            self.clf_task\n",
    "        )\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        return self.model(embeddings)\n",
    "    \n",
    "CIS_NET = CIS(\"hi\")\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "class TrainTest:\n",
    "\t\"\"\"\n",
    "\tTrains the Convolutional Neural Network (CNN)\n",
    "\tArgs: \n",
    "\t\ttrain: A tensor representing training samples of benign and spectre embeddings\n",
    "\t\ttest: A tensor representing testing samples of benign and spectre embeddings\n",
    "\t\"\"\"\n",
    "    \n",
    "\tdef __init__(self) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.CRITERION = nn.BCELoss()\n",
    "\t\tself.OPTIMIZER = optim.Adam(CIS_NET.parameters(), lr =  0.0001, betas = (0.5, 0.999))\n",
    "\t\tself.VALIDATION_LOSS_MIN = np.inf\n",
    "\t\tself.TRAIN_LOSSES, self.TRAIN_ACCURACIES, self.VALIDATION_LOSSES, self.VALIDATION_ACCURACIES = ([] for _ in range(4))\n",
    "\t\tself.EPOCH_TRAIN_LOSSES, self.EPOCH_TRAIN_ACCURACIES, self.EPOCH_VALIDATION_LOSSES, self.EPOCH_VALIDATION_ACCURACIES = ([] for _ in range(4))\n",
    "    \n",
    "\tdef accuracy(self):\n",
    "\t\taccurate_predictions = 0\n",
    "\t\t# _, _, loader_test = self.data_loader()\n",
    "\t\tCIS_NET.eval()\n",
    "\t\tfor embeddings, labels in loader_test:\n",
    "\t\t\toutputs = CIS_NET(embeddings.unsqueeze(1).float())\n",
    "\t\t\tpredictions = torch.round(outputs.squeeze()).eq(labels.float().view_as(torch.round(outputs.squeeze())))\n",
    "\t\t\taccurate_predictions += np.sum(np.squeeze(predictions.numpy()))\n",
    "\t\treturn accurate_predictions / len(loader_test.dataset) * 100\n",
    "    \n",
    "\tdef train(self):\n",
    "\t\tstart_time = time.time()\n",
    "\t\tprint(\">_ training\")\n",
    "\t\tfor epoch in range(3):\n",
    "\t\t\ttraining_loss = []\n",
    "\t\t\tvalidation_loss = []\n",
    "\n",
    "\t\t\tCIS_NET.train()\n",
    "\t\t\tfor idx, batch in tqdm(enumerate(loader_train, 0)):\n",
    "\t\t\t\tembeddings, labels = batch\n",
    "\t\t\t\tself.OPTIMIZER.zero_grad()\n",
    "\n",
    "\t\t\t\toutputs = CIS_NET(embeddings.unsqueeze(1).float())\n",
    "\t\t\t\tloss = self.CRITERION(outputs.squeeze(), labels.float())\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\tself.OPTIMIZER.step()\n",
    "\t\t\t\ttraining_loss.append(loss.item())\n",
    "\t\t\t\tif idx % 10 == 0: \n",
    "\t\t\t\t\tprint(f\"Training Loss = {loss.item()}\")\n",
    "\t\t\t\t\tself.TRAIN_LOSSES.append(loss.item())\n",
    "\t\t\t\t\tprint(f\"Training Accuracy = {self.accuracy()}\")\n",
    "\t\t\t\t\tself.TRAIN_ACCURACIES.append(self.accuracy())\n",
    "\n",
    "\t\t\tself.EPOCH_TRAIN_LOSSES.append(loss.item())\n",
    "\t\t\tself.EPOCH_TRAIN_ACCURACIES.append(self.accuracy())\n",
    "\n",
    "\t\t\tCIS_NET.eval()\n",
    "\t\t\tfor idx, batch in tqdm(enumerate(loader_val, 0)):\n",
    "\t\t\t\tembeddings, labels = batch\n",
    "\t\t\t\toutputs = CIS_NET(embeddings.unsqueeze(1).float())\n",
    "\t\t\t\tloss = self.CRITERION(outputs.squeeze(), labels.float())\n",
    "\t\t\t\tvalidation_loss.append(loss.item())\n",
    "\t\t\t\tif idx % 10 == 0:\n",
    "\t\t\t\t\tprint(f\"Validation Loss = {loss.item()}\")\n",
    "\t\t\t\t\tself.VALIDATION_LOSSES.append(loss.item())\n",
    "\t\t\t\t\tprint(f\"Validation Accuracy = {self.accuracy()}\")\n",
    "\t\t\t\t\tself.VALIDATION_ACCURACIES.append(self.accuracy())\n",
    "\n",
    "\t\t\tself.EPOCH_VALIDATION_LOSSES.append(loss.item())\n",
    "\t\t\tself.EPOCH_VALIDATION_ACCURACIES.append(self.accuracy())\n",
    "\n",
    "\t\t\tprint(f\"Average training loss: {np.mean(training_loss)} \\nAverage validation loss: {np.mean(validation_loss)}\")\n",
    "\t\t\tprint(f\"Epoch: {epoch}\")\n",
    "\t\t\tprint(f\"Finshed training. \\nTime to train: {time.time() - start_time} seconds\")\n",
    "            \n",
    "\t\t# torch.save(CIS_NET.state_dict(), \"./n_model.pth\")\n",
    "\n",
    "# \tdef pickle(self, data, file_name):\n",
    "# \t\twith open(file_name, 'wb') as file:\n",
    "# \t\t\tpickle.dump(data, file)\n",
    "        \n",
    "# \tdef store_metrics(self):\n",
    "# \t\tn_metrics = {\n",
    "# \t\t\t\"train_losses\" : self.TRAIN_LOSSES, \n",
    "# \t\t\t\"train_accuracies\" : self.TRAIN_ACCURACIES,\n",
    "# \t\t\t\"validation_losses\" : self.VALIDATION_LOSSES,\n",
    "# \t\t\t\"validation_accuracies\" : self.VALIDATION_ACCURACIES,\n",
    "# \t\t\t\"epoch_train_losses\" : self.EPOCH_TRAIN_LOSSES,\n",
    "# \t\t\t\"epoch_train_accuracies\" : self.EPOCH_TRAIN_ACCURACIES,\n",
    "# \t\t\t\"epoch_validation_losses\" : self.EPOCH_VALIDATION_LOSSES,\n",
    "# \t\t\t\"epoch_validation_accuracies\" : self.EPOCH_VALIDATION_ACCURACIES\n",
    "# \t\t}\n",
    "# \t\t# self.pickle(n_metrics, \"./n_metrics.pickle\")\n",
    "\n",
    "        \n",
    "tt = TrainTest()\n",
    "tt.train()\n",
    "# tt.test()\n",
    "# tt.store_metrics()\n",
    "\n",
    "# start_time = time.time()\n",
    "# labels_metric = []\n",
    "# prediction_metric = []\n",
    "# num_correct = 0\n",
    "# CIS_NET.eval()\n",
    "# start_time = time.time()\n",
    "# with torch.no_grad():\n",
    "# \tfor embeddings, labels in loader_test:\n",
    "# \t\toutputs = CIS_NET(embeddings.unsqueeze(1).float())\n",
    "# \t\tpredictions = torch.round(outputs.squeeze())\n",
    "# \t\tprediction_metric.append(predictions.tolist())\n",
    "# \t\tlabels_metric.append(labels.tolist())\n",
    "# \t\tcorrect_tensor = predictions.eq(labels.float().view_as(predictions))\n",
    "# \t\tcorrect = np.squeeze(correct_tensor.numpy())\n",
    "# \t\tnum_correct += np.sum(correct)\n",
    "# \ttest_acc = num_correct/len(loader_test.dataset)\n",
    "# \tprint(f\"Accuracy: {test_acc}\")\n",
    "# print(f\"Finshed Predicting. \\nTime to predict: {time.time() - start_time} seconds\")\n",
    "# # print(len(labels_metric))\n",
    "\n",
    "\n",
    "# # test_losses = [] # track loss\n",
    "# # num_correct = 0\n",
    "# # criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "# # CIS_NET.eval()\n",
    "# # # iterate over test data\n",
    "# # for inputs, labels in loader_test:\n",
    "    \n",
    "# #     # get predicted outputs\n",
    "# #     output = CIS_NET(inputs.unsqueeze(1))\n",
    "    \n",
    "# #     # calculate loss\n",
    "# #     test_loss = criterion(output.squeeze(), labels.float())\n",
    "# #     test_losses.append(test_loss.item())\n",
    "    \n",
    "# #     # convert output probabilities to predicted class (0 or 1)\n",
    "# #     pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "# #     # compare predictions to true label\n",
    "# #     correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "# #     correct =  np.squeeze(correct_tensor.cpu().numpy())\n",
    "# #     num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# # # -- stats! -- ##\n",
    "# # # avg test loss\n",
    "# # print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# # # accuracy over all test data\n",
    "# # test_acc = num_correct / len(loader_test.dataset)\n",
    "# # print(\"Test accuracy: {:.3f}\".format(test_acc))\n",
    "\n",
    "# # optimizer = optim.Adam(CIS_NET.parameters(), lr =  0.0001, betas = (0.5, 0.999))\n",
    "# # criterion = nn.BCELoss()\n",
    "# # def train():\n",
    "# #     counter = 0\n",
    "# #     CIS_NET.train()\n",
    "# #     for epoch in range(3):\n",
    "# #         for inputs, labels in loader_train:\n",
    "# #             counter += 1\n",
    "# #             optimizer.zero_grad() # zero accumulated units\n",
    "# #             output = CIS_NET(inputs) # get the output from the model\n",
    "# #             loss = criterion(outputs.squeeze(), labels.float()) # calculate loss\n",
    "# #             loss.backward() # backpropagate\n",
    "# #             optimizer.step()\n",
    "# #             if counter % 100 == 0:\n",
    "# #                 val_losses = []\n",
    "# #                 CIS_NET.eval()\n",
    "# #                 for inputs, labels in loader_val:\n",
    "# #                     output = CIS_NET(inputs)\n",
    "# #                     val_loss = criterion(outputs.squeeze(), labels.float()) # calculate loss\n",
    "# #                     val_losses.apend(val_loss.item())\n",
    "# #                 CIS_NET.train()\n",
    "# #                 print(f\"Epoch: {epoch + 1}/{epoch} \\\\ Step: {counter}, \\\\ Loss: {loss.item()}, \\\\ Val Loss: {np.mean(val_losses)}\")\n",
    "\n",
    "# # train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
